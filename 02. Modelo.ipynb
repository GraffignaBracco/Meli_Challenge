{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_meli import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_data_evaluate(json_dic_i):\n",
    "    '''\n",
    "    Esta función sirve para extraer los datos de un diccionario con la información\n",
    "    de un item. Transforma los Tags en variables dummies y filtra solamente las \n",
    "    variables que se van a usar en el modelo\n",
    "    \n",
    "    '''\n",
    "    tag_list = ['ahora-12','brand_verified','cart_eligible','dragged_bids_and_visits',\n",
    "                'good_quality_picture','good_quality_thumbnail','immediate_payment',\n",
    "                'incomplete_technical_specs','loyalty_discount_eligible','shipping_guaranteed',\n",
    "                'poor_quality_picture','poor_quality_thumbnail','catalog_listing_eligible',\n",
    "                'extended_warranty_eligible','lightning_deal','under_infractions','supermarket_eligible',\n",
    "                'dragged_visits','deal_of_the_day','catalog_forewarning','only_html_description','hirable']\n",
    "\n",
    "    seller_list = ['brand','credits_active_borrower','credits_priority_2','credits_priority_4','credits_profile',\n",
    "                   'developer','eshop','large_seller','medium_seller','messages_as_buyer','messages_as_seller',\n",
    "                   'mshops','normal','user_info_verified','car_dealer','medium_seller_advanced','credits_priority_3',\n",
    "                   'credits_priority_1','credits_open_market','ngo','from_facebook','real_estate_agency']\n",
    "\n",
    "    ship_list = ['fs_removed_by_tagger','fulfillment','mandatory_free_shipping','self_service_in','self_service_out',\n",
    "                'fbm_in_process','is_flammable','me2_blocked','me2_available', 'fbm_in_progress', 'fbm_me2_frozen',\n",
    "                'adoption_required']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "            \n",
    "\n",
    "    seller_url = 'https://api.mercadolibre.com/users/' + str(json_dic_i.get('seller_id'))\n",
    "    api_request_seller = requests.get(seller_url)   \n",
    "    json_dic_i_seller = json.loads(api_request_seller.content) \n",
    "        \n",
    "\n",
    "    dummies_tag_dic = {i:int(i in json_dic_i.get('tags')) for i in tag_list}\n",
    "\n",
    "    dummies_seller_dic = {i:int(i in json_dic_i_seller.get('tags')) for i in seller_list}\n",
    "\n",
    "    dummies_ship_dic = {i:int(i in json_dic_i.get('shipping').get('tags')) for i in ship_list}\n",
    "\n",
    "    if json_dic_i.get('original_price') is None:\n",
    "        descuento = 0\n",
    "    else:\n",
    "        descuento = (json_dic_i.get('original_price')-json_dic_i.get('original_price'))/json_dic_i.get('original_price')\n",
    "    \n",
    "    if json_dic_i.get('condition') is None:\n",
    "        condition = 'not_specified'\n",
    "    else:\n",
    "        condition = json_dic_i.get('condition')\n",
    "    \n",
    "    if json_dic_i.get('shipping').get('logistic_type') is None:\n",
    "        shipping_logistic_type = 'not_specified'\n",
    "    else:\n",
    "        shipping_logistic_type = json_dic_i.get('shipping').get('logistic_type')    \n",
    "        \n",
    "    dic = {'category_id2': json_dic_i.get('category_id'),\n",
    "           'price': json_dic_i.get('price'),\n",
    "           'available_quantity':json_dic_i.get('available_quantity'),\n",
    "           'sold_quantity':json_dic_i.get('sold_quantity'),\n",
    "           'buying_mode':json_dic_i.get('buying_mode'),\n",
    "           'listing_type_id':json_dic_i.get('listing_type_id'),\n",
    "           'condition' : condition,\n",
    "           'accepts_mercadopago':int(json_dic_i.get('accepts_mercadopago')),\n",
    "           'descuento':descuento,\n",
    "           'free_shipping':int(json_dic_i.get('shipping').get('free_shipping')),\n",
    "           'shipping_mode':json_dic_i.get('shipping').get('mode'),\n",
    "           'shipping_logistic_type': shipping_logistic_type,\n",
    "           'shipping_store_pick_up': int(json_dic_i.get('shipping').get('store_pick_up')),\n",
    "           'seller_transactions_ratings_negative': json_dic_i_seller.get('seller_reputation').get('transactions').get('ratings').get('negative'),\n",
    "           'seller_transactions_ratings_neutral':json_dic_i_seller.get('seller_reputation').get('transactions').get('ratings').get('neutral'),\n",
    "           'seller_transactions_ratings_positive':json_dic_i_seller.get('seller_reputation').get('transactions').get('ratings').get('positive'),\n",
    "           'seller_transactions_total':json_dic_i_seller.get('seller_reputation').get('transactions').get('total'),\n",
    "           'seller_level_id':json_dic_i_seller.get('seller_reputation').get('level_id'),\n",
    "           'seller_power_seller_status':json_dic_i_seller.get('seller_reputation').get('power_seller_status'),\n",
    "           'seller_transactions_canceled': json_dic_i_seller.get('seller_reputation').get('transactions').get('canceled'),\n",
    "           'seller_transactions_completed':json_dic_i_seller.get('seller_reputation').get('transactions').get('completed'),\n",
    "           'seller_years_from_registration':years_between_date_and_today(json_dic_i_seller.get('registration_date'))}\t\n",
    "\n",
    "    tmp = pd.DataFrame({**dic, **dummies_tag_dic, **dummies_seller_dic, **dummies_ship_dic}, index=[json_dic_i.get('id')])\n",
    "    df = pd.concat([df,tmp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpute_prices(df, price_by_category):\n",
    "  '''\n",
    "  Inputa los valores nulos de la variable de precios con la media de\n",
    "  por categoría de X_Train\n",
    "  '''\n",
    "  for i in range(len(price_by_category)):\n",
    "    price_i = price_by_category.loc[i,'price']\n",
    "    df.loc[(df.category_id == price_by_category.loc[i,'category_id'])&(df.price.isnull()), 'price'] = price_i\n",
    "  \n",
    "  df.loc[df.seller_power_seller_status.isnull(), 'seller_power_seller_status'] = 'None'\n",
    "  df.loc[df.seller_level_id.isnull(), 'seller_level_id'] = 'None'\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode X_train\n",
    "def encode_x_train(x):\n",
    "    '''\n",
    "    Transforma las variables categóricas en dummies.\n",
    "    También crea dos listas con los nombres de las variables dummies para que puedan\n",
    "    ser usadas para encodear el dataset de testing\n",
    "    '''\n",
    "    #Columnas categoricas y string\n",
    "    columns_to_encode = list(x.select_dtypes(include=['category','object']))\n",
    "    \n",
    "    #Dataframe encodeado\n",
    "    encoded_x_train = pd.get_dummies(x[columns_to_encode])\n",
    "    \n",
    "    #Columnas\n",
    "    encoded_columns = encoded_x_train.columns\n",
    "    \n",
    "    #Concatenación\n",
    "    x_encoded = pd.concat([x.drop(columns_to_encode, axis=1), encoded_x_train], axis=1)\n",
    "    \n",
    "    total_columns = x_encoded.columns\n",
    "    \n",
    "    return (x_encoded, columns_to_encode, encoded_columns, total_columns)\n",
    "    \n",
    "def encode_x_test(x, columns_to_encode, encoded_columns, total_columns):\n",
    "    '''\n",
    "    Encodea el dataset de testing con las listas generadas por\n",
    "    la función \"encode_x_train\" para asegurar que ambos datasets\n",
    "    tengan las mismas variables\n",
    "    '''\n",
    "    #Transformación de variables categoricas en dummies\n",
    "    x_dummies = pd.get_dummies(x[columns_to_encode])\n",
    "\n",
    "    #Nombres de las nuevas variables dummies\n",
    "    x_test_encoded_columns = x_dummies.columns\n",
    "\n",
    "    #Variables dummies que se encuentran en el dataset de train pero no de test\n",
    "    missing_columns = [x for x in encoded_columns if x not in x_test_encoded_columns]\n",
    "\n",
    "    for columns in missing_columns:\n",
    "        x_dummies[columns] = 0\n",
    "    \n",
    "    \n",
    "    #Concatenación de los dataset\n",
    "    x_test = pd.concat([x.drop(columns_to_encode, axis=1), x_dummies], axis=1).fillna(0)\n",
    "    \n",
    "    #Eliminación de todas las columnas que no estén en X train\n",
    "    x_test = x_test.loc[:, x_test.columns.isin(total_columns)]\n",
    "    \n",
    "    #Dejar el mismo orden\n",
    "    x_test = x_test[total_columns]\n",
    "    \n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(dataset_path):\n",
    "    '''\n",
    "    Lee un dataframe, lo procesa y lo divide en Training y Testing\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Lectura del dataframe\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    #Crea una variable con las categorías para hacer un split balanceado\n",
    "    categories = df.category_id\n",
    "    \n",
    "    X = df.drop(['id','category_id2', 'sold_quantity'], axis=1, errors='ignore')\n",
    "    y = df.sold_quantity    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=9, test_size=0.2, stratify = categories)\n",
    "    \n",
    "    #Calcula el precio mediano por categoría\n",
    "    median_by_category = X_train.groupby('category_id').price.median().reset_index()\n",
    "    \n",
    "    #Imputa los precios medianos \n",
    "    X_train_imputed = inpute_prices(X_train, median_by_category)\n",
    "    X_test_imputed = inpute_prices(X_test, median_by_category)\n",
    "    \n",
    "    #One Hot Encoding\n",
    "    X_train_encoded, columns_to_encode, encoded_columns, total_columns = encode_x_train(X_train_imputed)\n",
    "    X_test_encoded = encode_x_test(X_test_imputed, columns_to_encode, encoded_columns, total_columns)\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded, y_train, y_test, categories, columns_to_encode, encoded_columns, total_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cv(params):\n",
    "    '''\n",
    "    Entrena un modelo XGBoost Regressor con Cross-Validation\n",
    "    '''\n",
    "    model = xgb.XGBRegressor(**params, silent=True)\n",
    "    kfold = KFold(n_splits=5, random_state=9, shuffle=True)\n",
    "    score = -1*cross_val_score(model, X_train_encoded, y_train, cv=kfold,scoring='neg_root_mean_squared_error').mean()\n",
    "    return {'loss': score, 'status': STATUS_OK}    \n",
    "    xg\n",
    "def optimize(trials, space, max_evals):\n",
    "    '''\n",
    "    Optimización de hiper parámetros\n",
    "    '''  \n",
    "    best = fmin(score_cv, space, algo=tpe.suggest, max_evals=max_evals)\n",
    "    return best\n",
    "\n",
    "def get_optimum_parameters(max_evals, parameters_path):\n",
    "    '''\n",
    "    Obtiene los hiperparámetros óptimos\n",
    "    '''\n",
    "    trials = Trials()\n",
    "    best_params = optimize(trials, space, max_evals)\n",
    "    \n",
    "    #Guarda los parametros con Pickle\n",
    "    f = open(r\"Models/\"+parameters_path+\".pkl\",\"wb\")\n",
    "    pickle.dump(best_parameters,f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xgboost_model(parameters_path):\n",
    "    '''\n",
    "    Lee los parámetros obtenidos por la función \"get_optimum_parameters\"\n",
    "    y devuelve un modelo con esos parámetros\n",
    "    '''\n",
    "    pickle_in = open(parameters_path,\"rb\")\n",
    "    best_parameters = pickle.load(pickle_in)\n",
    "    model = xgb.XGBRegressor(**best_parameters, silent=True)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_xgboost(model):\n",
    "    '''\n",
    "    Calcula el MAE y RMSE por categoría de un modelo XGBoostRegressor\n",
    "    '''\n",
    "    \n",
    "    reporte_final = pd.DataFrame()\n",
    "    \n",
    "    for i in categories.unique():\n",
    "\n",
    "        indice_train_i = X_train.loc[X_train.category_id==i].index\n",
    "\n",
    "        indice_test_i = X_test.loc[X_test.category_id==i].index\n",
    "\n",
    "        y_train_predict_i = model.predict(X_train_encoded.loc[indice_train_i, :])\n",
    "\n",
    "        y_test_predict_i  = model.predict(X_test_encoded.loc[indice_test_i, :])\n",
    "\n",
    "        y_train_i = y_train.loc[indice_train_i]\n",
    "\n",
    "        y_test_i = y_test.loc[indice_test_i]\n",
    "\n",
    "        rmse_train_i = np.sqrt(mean_squared_error(y_train_predict_i, y_train_i))\n",
    "\n",
    "        rmse_test_i  = np.sqrt(mean_squared_error(y_test_predict_i, y_test_i))\n",
    "\n",
    "        mae_train_i  = mean_absolute_error(y_train_predict_i, y_train_i)\n",
    "\n",
    "        mae_test_i   = mean_absolute_error(y_test_predict_i, y_test_i)\n",
    "\n",
    "        tmp = pd.DataFrame({'category_id': i, 'modelo': 'XGBoostRegressor',\n",
    "                            'rmse_train': rmse_train_i, 'rmse_test': rmse_test_i,\n",
    "                            'mae_train' : mae_train_i, 'mae_test' : mae_test_i}, index=[i])\n",
    "\n",
    "        reporte_final = pd.concat([reporte_final, tmp], axis=0)\n",
    "    \n",
    "    return model, reporte_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_item(item):\n",
    "    '''\n",
    "    Obtiene la predicción de Sold Quantity para un Item e\n",
    "    '''\n",
    "    URL = \"https://api.mercadolibre.com/items/\" + item\n",
    "    request = requests.get(URL) \n",
    "    json_dic_i = json.loads(request.content)\n",
    "\n",
    "    category_url = \"https://api.mercadolibre.com/categories/\"+ json_dic_i.get('category_id')\n",
    "    request_category = requests.get(category_url) \n",
    "    json_dic_category = json.loads(request_category.content)\n",
    "\n",
    "    tmp = get_item_data_evaluate(json_dic_i)\n",
    "    tmp['category_id'] = json_dic_category.get('path_from_root')[0].get('id')\n",
    "\n",
    "    X = tmp.drop(['id','category_id2', 'sold_quantity'], axis=1, errors='ignore')\n",
    "    y = tmp.sold_quantity  \n",
    "\n",
    "    item_imputed = inpute_prices(X, median_by_category)\n",
    "    item_encoded = encode_x_test(item_imputed, columns_to_encode, encoded_columns, total_columns)\n",
    "\n",
    "    y_pred = model.predict(item_encoded)\n",
    "\n",
    "    rmse  = np.sqrt(mean_squared_error(y_pred, y))\n",
    "\n",
    "    mae   = mean_absolute_error(y_pred, y)\n",
    "\n",
    "    results = {'item': item, 'y': y[0], 'y_pred': y_pred[0], 'rmse': rmse, 'mae': mae}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de Hiperparámetros Óptimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:965: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:44:45] WARNING: /workspace/src/learner.cc:480:   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:44:50] WARNING: /workspace/src/learner.cc:480:   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:44:55] WARNING: /workspace/src/learner.cc:480:   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:00] WARNING: /workspace/src/learner.cc:480:   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:04] WARNING: /workspace/src/learner.cc:480:   \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:23<00:00, 23.38s/trial, best loss: 956.1763644206679]\n"
     ]
    }
   ],
   "source": [
    "X_train_encoded, X_test_encoded, y_train, y_test, categories, columns_to_encode, encoded_columns, total_columns = process_dataframe(\"Data/dataset.csv\")\n",
    "\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(5, 30, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(10, 200, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 1.0, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(5, 55, 6, dtype=int)),\n",
    "        'subsample':hp.quniform('subsample', 0.7, 0.9, 0.1),\n",
    "        'eta':hp.quniform('eta', 0.1, 0.3, 0.1),       \n",
    "        'objective':'reg:squarederror',         \n",
    "        'eval_metric': 'rmse',\n",
    "        'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.40, 0.05)),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'n_estimators': hp.choice('n_estimators', np.arange(10, 200, 10, dtype=int)),\n",
    "        'seed': 9,\n",
    "    }\n",
    "\n",
    "get_optimum_parameters(1, \"XGBoost_parameters_prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de performance del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:40:27] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_xgboost_model(\"Models/XGBoost_parameters.pkl\")\n",
    "reporte_final2 = score_xgboost(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de Item en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"MLA842101865\"\n",
    "evaluate_item(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
